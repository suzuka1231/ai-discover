#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Discover - è¨˜äº‹è‡ªå‹•åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ç”ŸæˆAIé–¢é€£ã®è¨˜äº‹ã‚’åé›†ã—ã€HTMLã‚µã‚¤ãƒˆã‚’æ›´æ–°ã—ã¾ã™
"""

import json
import re
import feedparser
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import time
import hashlib

class ArticleCollector:
    def __init__(self, config_path='config.json'):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.articles = []
        self.seen_urls = set()
        
    def fetch_all_sources(self):
        """å…¨ã¦ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’åé›†"""
        print("ğŸ“¡ è¨˜äº‹ã®åé›†ã‚’é–‹å§‹...")
        
        for source in self.config['sources']:
            try:
                print(f"  â†³ {source['name']} ã‚’å–å¾—ä¸­...")
                
                if source['type'] == 'rss':
                    self.fetch_rss(source)
                elif source['type'] == 'note_rss':
                    self.fetch_note_api(source)
                    
                time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                
            except Exception as e:
                print(f"  âš ï¸  ã‚¨ãƒ©ãƒ¼: {source['name']} - {str(e)}")
        
        # æ‰‹å‹•è¿½åŠ è¨˜äº‹ã‚’è¿½åŠ 
        self.add_manual_articles()
        
        print(f"âœ… åˆè¨ˆ {len(self.articles)} ä»¶ã®è¨˜äº‹ã‚’åé›†")
        
    def fetch_rss(self, source):
        """æ¨™æº–RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—"""
        feed = feedparser.parse(source['url'])
        
        for entry in feed.entries:
            # æ—¥ä»˜ãƒã‚§ãƒƒã‚¯
            if not self.is_recent(entry):
                continue
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if not self.matches_keywords(entry, source['keywords']):
                continue
            
            # URLã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
            url = entry.link
            if url in self.seen_urls or url in self.config.get('blacklist', []):
                continue
            
            # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            article = self.create_article_data(entry, source)
            if article:
                self.articles.append(article)
                self.seen_urls.add(url)
    
    def fetch_note_api(self, source):
        """noteã®APIã‹ã‚‰è¨˜äº‹ã‚’å–å¾—"""
        try:
            # noteã®ç‰¹å®šã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ã®RSSã‚’å–å¾—
            # URLã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’æŠ½å‡º
            match = re.search(r'creators/([^/]+)/', source['url'])
            if not match:
                return
            
            username = match.group(1)
            rss_url = f"https://note.com/{username}/rss"
            
            feed = feedparser.parse(rss_url)
            
            for entry in feed.entries:
                if not self.is_recent(entry):
                    continue
                
                if not self.matches_keywords(entry, source['keywords']):
                    continue
                
                url = entry.link
                if url in self.seen_urls or url in self.config.get('blacklist', []):
                    continue
                
                article = self.create_article_data(entry, source)
                if article:
                    self.articles.append(article)
                    self.seen_urls.add(url)
                    
        except Exception as e:
            print(f"    note API ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def create_article_data(self, entry, source):
        """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–"""
        try:
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
            title = entry.get('title', '').strip()
            description = self.extract_description(entry)
            
            # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã‚’å–å¾—
            thumbnail = self.extract_thumbnail(entry, source['platform'])
            
            # AIãƒ„ãƒ¼ãƒ«ã‚’è‡ªå‹•æ¤œå‡º
            ai_tools = self.detect_ai_tools(title + ' ' + description)
            
            # ã‚¿ã‚°ã‚’ç”Ÿæˆ
            tags = self.generate_tags(title, description, ai_tools)
            
            # å…¬é–‹æ—¥æ™‚
            published = self.get_published_time(entry)
            
            article = {
                'id': hashlib.md5(entry.link.encode()).hexdigest()[:8],
                'title': title,
                'description': description,
                'url': entry.link,
                'platform': source['platform'],
                'author': self.extract_author(entry),
                'time': self.format_time_ago(published),
                'timestamp': published.isoformat() if published else None,
                'thumbnail': thumbnail,
                'tags': tags,
                'aiTools': ai_tools
            }
            
            return article
            
        except Exception as e:
            print(f"    è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def extract_description(self, entry):
        """è¨˜äº‹ã®èª¬æ˜æ–‡ã‚’æŠ½å‡º"""
        # summary ã¾ãŸã¯ description ã‹ã‚‰å–å¾—
        desc = entry.get('summary', entry.get('description', ''))
        
        # HTMLã‚¿ã‚°ã‚’é™¤å»
        if desc:
            soup = BeautifulSoup(desc, 'html.parser')
            desc = soup.get_text().strip()
            # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
            if len(desc) > 200:
                desc = desc[:197] + '...'
        
        return desc
    
    def extract_thumbnail(self, entry, platform):
        """ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã‚’æŠ½å‡º"""
        # media:thumbnail ã¾ãŸã¯ enclosure ã‹ã‚‰å–å¾—
        if hasattr(entry, 'media_thumbnail'):
            return entry.media_thumbnail[0]['url']
        
        if hasattr(entry, 'enclosures') and entry.enclosures:
            for enclosure in entry.enclosures:
                if 'image' in enclosure.get('type', ''):
                    return enclosure['href']
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ã®æœ€åˆã®ç”»åƒã‚’æ¢ã™
        content = entry.get('content', [{}])[0].get('value', '')
        if content:
            soup = BeautifulSoup(content, 'html.parser')
            img = soup.find('img')
            if img and img.get('src'):
                return img['src']
        
        # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆ¥ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç”»åƒ
        return self.get_default_thumbnail(platform)
    
    def get_default_thumbnail(self, platform):
        """ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆ¥ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚µãƒ ãƒã‚¤ãƒ«"""
        defaults = {
            'note': 'data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iODAwIiBoZWlnaHQ9IjU2MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iODAwIiBoZWlnaHQ9IjU2MCIgZmlsbD0iIzQxYzliNCIvPjx0ZXh0IHg9IjUwJSIgeT0iNTAlIiBmb250LWZhbWlseT0iQXJpYWwiIGZvbnQtc2l6ZT0iNDgiIGZpbGw9IndoaXRlIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiBkeT0iLjNlbSI+bm90ZTwvdGV4dD48L3N2Zz4=',
            'zenn': 'data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iODAwIiBoZWlnaHQ9IjU2MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iODAwIiBoZWlnaHQ9IjU2MCIgZmlsbD0iIzNlYThmZiIvPjx0ZXh0IHg9IjUwJSIgeT0iNTAlIiBmb250LWZhbWlseT0iQXJpYWwiIGZvbnQtc2l6ZT0iNDgiIGZpbGw9IndoaXRlIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiBkeT0iLjNlbSI+WmVubjwvdGV4dD48L3N2Zz4=',
            'blog': 'data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iODAwIiBoZWlnaHQ9IjU2MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cmVjdCB3aWR0aD0iODAwIiBoZWlnaHQ9IjU2MCIgZmlsbD0iIzY2NjY2NiIvPjx0ZXh0IHg9IjUwJSIgeT0iNTAlIiBmb250LWZhbWlseT0iQXJpYWwiIGZvbnQtc2l6ZT0iNDgiIGZpbGw9IndoaXRlIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiBkeT0iLjNlbSI+QmxvZzwvdGV4dD48L3N2Zz4='
        }
        return defaults.get(platform, defaults['blog'])
    
    def extract_author(self, entry):
        """è‘—è€…åã‚’æŠ½å‡º"""
        return entry.get('author', entry.get('dc:creator', 'ä¸æ˜'))
    
    def get_published_time(self, entry):
        """å…¬é–‹æ—¥æ™‚ã‚’å–å¾—"""
        for time_field in ['published_parsed', 'updated_parsed']:
            if hasattr(entry, time_field):
                time_struct = getattr(entry, time_field)
                if time_struct:
                    return datetime(*time_struct[:6])
        return datetime.now()
    
    def format_time_ago(self, pub_time):
        """ç›¸å¯¾æ™‚é–“è¡¨è¨˜ã«å¤‰æ›"""
        if not pub_time:
            return 'ä¸æ˜'
        
        now = datetime.now()
        diff = now - pub_time
        
        if diff.days > 30:
            return f"{diff.days // 30}ãƒ¶æœˆå‰"
        elif diff.days > 0:
            return f"{diff.days}æ—¥å‰"
        elif diff.seconds > 3600:
            return f"{diff.seconds // 3600}æ™‚é–“å‰"
        else:
            return f"{diff.seconds // 60}åˆ†å‰"
    
    def is_recent(self, entry):
        """æœ€è¿‘ã®è¨˜äº‹ã‹ãƒã‚§ãƒƒã‚¯"""
        pub_time = self.get_published_time(entry)
        if not pub_time:
            return True
        
        days_limit = self.config['settings']['days_to_keep']
        cutoff = datetime.now() - timedelta(days=days_limit)
        
        return pub_time >= cutoff
    
    def matches_keywords(self, entry, source_keywords):
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°"""
        title = entry.get('title', '').lower()
        description = self.extract_description(entry).lower()
        content = title + ' ' + description
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        global_filter = self.config['keywords_filter']
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        for exclude in global_filter['exclude']:
            if exclude.lower() in content:
                return False
        
        # å¿…é ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆã©ã‚Œã‹1ã¤å«ã‚€ï¼‰
        all_keywords = source_keywords + global_filter['required_any']
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯å…¨ã¦è¨±å¯
        if not all_keywords:
            return True
        
        for keyword in all_keywords:
            if keyword.lower() in content:
                return True
        
        return False
    
    def detect_ai_tools(self, text):
        """AIãƒ„ãƒ¼ãƒ«ã‚’è‡ªå‹•æ¤œå‡º"""
        text_lower = text.lower()
        tools = []
        
        ai_patterns = {
            'chatgpt': ['chatgpt', 'gpt-4', 'gpt-3', 'gpt4', 'gpt3'],
            'claude': ['claude', 'anthropic'],
            'gemini': ['gemini', 'bard', 'google ai'],
            'sora': ['sora', 'openai sora'],
            'midjourney': ['midjourney', 'mj'],
            'stable-diffusion': ['stable diffusion', 'sd'],
            'dall-e': ['dall-e', 'dalle']
        }
        
        for tool, patterns in ai_patterns.items():
            for pattern in patterns:
                if pattern in text_lower:
                    tools.append(tool)
                    break
        
        return list(set(tools))  # é‡è¤‡é™¤å»
    
    def generate_tags(self, title, description, ai_tools):
        """ã‚¿ã‚°ã‚’è‡ªå‹•ç”Ÿæˆ"""
        text = (title + ' ' + description).lower()
        tags = []
        
        # AI ãƒ„ãƒ¼ãƒ«åã‚’ã‚¿ã‚°ã«
        tool_names = {
            'chatgpt': 'ChatGPT',
            'claude': 'Claude',
            'gemini': 'Gemini',
            'sora': 'Sora',
            'midjourney': 'Midjourney'
        }
        
        for tool in ai_tools:
            if tool in tool_names:
                tags.append(tool_names[tool])
        
        # ã‚«ãƒ†ã‚´ãƒªã‚¿ã‚°
        if any(word in text for word in ['ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ', 'prompt']):
            tags.append('ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ')
        
        if any(word in text for word in ['ç”»åƒç”Ÿæˆ', 'image generation', 'ç”»åƒ']):
            tags.append('ç”»åƒç”Ÿæˆ')
        
        if any(word in text for word in ['å‹•ç”»ç”Ÿæˆ', 'video generation', 'å‹•ç”»', 'sora']):
            tags.append('å‹•ç”»ç”Ÿæˆ')
        
        if any(word in text for word in ['gptä½œæˆ', 'gpts', 'ã‚«ã‚¹ã‚¿ãƒ gpt']):
            tags.append('GPTä½œæˆ')
        
        if any(word in text for word in ['åŠ¹ç‡åŒ–', 'è‡ªå‹•åŒ–', 'automation']):
            tags.append('åŠ¹ç‡åŒ–')
        
        # æœ€å¤§5ã¤ã¾ã§
        return tags[:5]
    
    def add_manual_articles(self):
        """æ‰‹å‹•è¿½åŠ è¨˜äº‹ã‚’èª­ã¿è¾¼ã‚€"""
        manual = self.config.get('manual_articles', [])
        for article in manual:
            if article.get('url') and article['url'] not in self.seen_urls:
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚‹ã‹ç¢ºèª
                if article.get('title'):
                    self.articles.append(article)
                    self.seen_urls.add(article['url'])
    
    def sort_and_limit(self):
        """è¨˜äº‹ã‚’ã‚½ãƒ¼ãƒˆã—ã¦ä»¶æ•°åˆ¶é™"""
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
        self.articles.sort(
            key=lambda x: x.get('timestamp', ''), 
            reverse=True
        )
        
        # æœ€å¤§ä»¶æ•°ã§åˆ¶é™
        max_articles = self.config['settings']['max_articles']
        self.articles = self.articles[:max_articles]
    
    def save_to_json(self, output_path='data/articles.json'):
        """JSONå½¢å¼ã§ä¿å­˜"""
        self.sort_and_limit()
        
        data = {
            'last_updated': datetime.now().isoformat(),
            'total': len(self.articles),
            'articles': self.articles
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ {output_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        return data

def main():
    print("ğŸ¤– AI Discover - è¨˜äº‹åé›†ã‚’é–‹å§‹")
    print("=" * 50)
    
    collector = ArticleCollector('config.json')
    collector.fetch_all_sources()
    data = collector.save_to_json('data/articles.json')
    
    print("=" * 50)
    print(f"âœ¨ å®Œäº†ï¼ {data['total']} ä»¶ã®è¨˜äº‹ã‚’åé›†ã—ã¾ã—ãŸ")

if __name__ == '__main__':
    main()
